# Pipeline 01-03: PDF Corpus Ingestion

## ðŸ”¢ Pipeline Sequence: `01.03`
**Execution Order**: This is sub-pipeline 3 of main pipeline 01 (Data Ingestion)
- **Previous**: 01.02 - Restructure UKB Showcase Data
- **Current**: 01.03 - PDF Corpus Ingestion  
- **Next**: 02.01 - Data Conversion

## ðŸ“‹ Purpose
This pipeline downloads research paper PDFs from multiple academic sources using a multi-stage approach. It searches OpenAlex, CORE, and other open access repositories, provides manual download assistance for hard-to-fetch papers, validates all PDFs, and removes duplicates to create a clean corpus for downstream processing.

## âš™ï¸ Hyperparameters

| Parameter | Default | File | Method/Line | Description |
|-----------|---------|------|-------------|-------------|
| REQUEST_TIMEOUT | 10 sec | 01-openalex_fetcher.py | search methods | API request timeout for OpenAlex calls |
| REQUEST_TIMEOUT | 15 sec | 02-core_fetcher.py | make_api_request_with_retry() | API request timeout for CORE calls |
| DOWNLOAD_TIMEOUT | 30 sec | All fetchers | download_pdf() | PDF download timeout |
| MIN_FILE_SIZE | 1000 bytes | All scripts | Various methods | Minimum valid PDF file size |
| SIMILARITY_THRESHOLD | 0.8 | 01-openalex_fetcher.py | search_by_title() | Title matching threshold for OpenAlex |
| SIMILARITY_THRESHOLD | 0.8 | 02-core_fetcher.py | search_by_title() | Title matching threshold for CORE |
| TITLE_SIMILARITY_THRESHOLD | 0.7 | 03-other_fetcher.py | search_pmc_by_title() | PMC title matching threshold |
| TEXT_SIMILARITY_THRESHOLD | 0.85 | 05-validate_dedupe.py | group_pdfs_by_identity() | Text-based deduplication threshold |
| MAX_FILENAME_LENGTH | 200 chars | All fetchers | create_filename() | Maximum PDF filename length |
| MAX_RETRIES | 5 | 02-core_fetcher.py | make_api_request_with_retry() | API request retry attempts |
| RATE_LIMIT_DELAY | 12 sec | 02-core_fetcher.py | process_publications() | Delay between CORE API calls |
| BATCH_SIZE | 10 | 04-manual_download_helper.py | display_urls_for_batch() | URLs displayed per batch |
| MIN_PAGE_COUNT | 2 | 05-validate_dedupe.py | validate_pdf() | Minimum pages for valid PDF |
| MIN_TEXT_LENGTH | 10 chars | 05-validate_dedupe.py | validate_pdf() | Minimum extractable text length |
| CONTENT_SAMPLE_SIZE | 5000 chars | 05-validate_dedupe.py | extract_text_from_pdf() | Text sample size for similarity |

## ðŸŽ² Seeds and Reproducibility

| Seed Name | Value | File | Purpose |
|-----------|-------|------|---------|
| N/A | N/A | N/A | No random seeds used - pipeline is deterministic |

This pipeline does not use randomization, ensuring completely reproducible results across runs.

## ðŸš€ Execution Process

### Build Sequence:
1. Set up environment variables in `.env` file
2. Build Docker image: `docker build -t pdf-corpus-ingester .`
3. Ensure input publications file exists
4. Create output directories
5. Run pipeline stages

### Execution Order:
1. **01-openalex_fetcher.py** - Downloads PDFs from OpenAlex database
2. **02-core_fetcher.py** - Downloads PDFs from CORE repository
3. **03-other_fetcher.py** - Downloads from Unpaywall, PMC, arXiv
4. **04-manual_download_helper.py** - Interactive assistant for manual downloads
5. **05-validate_dedupe.py** - Validates and deduplicates all PDFs

## ðŸ¤ Manual Download Helper (Stage 4)

The **manual download helper** (`04-manual_download_helper.py`) is a key innovation that bridges the gap between automated and manual PDF acquisition. When automated systems can't download certain papers (due to CAPTCHAs, login requirements, or complex download mechanisms), this tool provides intelligent assistance.

### Why Manual Downloads Are Needed
Despite sophisticated automated fetchers, some PDFs require human intervention:
- **Login walls**: Publisher sites requiring institutional access
- **CAPTCHAs**: Anti-bot protection systems
- **Complex download flows**: Multi-step download processes
- **JavaScript-heavy sites**: Dynamic content that automation can't handle
- **Rate limiting**: Sites that block automated requests

### How the Helper Works

#### 1. **Smart URL Organization**
- Reads `manual_download.txt` (generated by previous stages)
- Groups URLs into manageable batches of 10
- Creates a beautiful web interface (`batch_urls.html`)
- Provides both batch and individual download options

#### 2. **Interactive Web Interface**
```
ðŸ“¥ PDF Download Manager
â”œâ”€â”€ Progress bar showing completion status
â”œâ”€â”€ Batch navigation (Previous 10 / Next 10)
â”œâ”€â”€ One-click "Open This Batch" (opens 10 tabs)
â”œâ”€â”€ Individual PDF links with context
â”œâ”€â”€ Download tracking with checkboxes
â””â”€â”€ Progress persistence across browser sessions
```

#### 3. **Smart File Management**
- **Auto-detection**: Scans download folder for new PDFs
- **Interactive matching**: Helps you match downloaded files to publications
- **Intelligent renaming**: Converts generic names like "download.pdf" to structured names
- **Progress tracking**: Remembers what you've completed

#### 4. **Workflow Example**
```bash
# 1. Setup (one time)
# Set browser download location to: ./output/manual/

# 2. Run helper
docker compose --profile stage4 up

# 3. Helper shows batches in terminal and opens web interface
# 4. Use web interface to download PDFs in batches
# 5. Return to terminal to rename downloaded files
# 6. Helper guides you through matching files to publications
# 7. Properly named files moved to ./output/manual_renamed/
```

### Key Features

- **Browser Integration**: Optimized for Chrome/Firefox download workflows
- **Resumable Sessions**: Can pause and continue later without losing progress
- **Batch Processing**: Handle 10 URLs at a time to avoid browser overload
- **Smart Naming**: Converts messy downloaded filenames to standard format
- **Progress Persistence**: Saves state in JSON file across sessions
- **Error Recovery**: Handles failed downloads gracefully

### Manual Helper Outputs
```
./output/manual/                    # Raw downloaded PDFs (generic names)
./output/manual_renamed/            # Properly renamed PDFs ready for validation
./output/manual_download_progress.json  # Session state and progress
./output/batch_urls.html            # Web interface for downloading
```

This tool typically recovers an additional 10-20% of PDFs that automated systems miss, significantly improving overall pipeline success rates.

### Script Execution:
```bash
# Option 1: Run individual stages
docker compose --profile stage1 up  # OpenAlex
docker compose --profile stage2 up  # CORE
docker compose --profile stage3 up  # Other sources
docker compose --profile stage4 up  # Manual helper
docker compose --profile stage5 up  # Validation

# Option 2: Automated pipeline (stages 1-3 + validation)
docker compose --profile automated up

# Option 3: Full pipeline including manual step
docker compose --profile full up
```

## ðŸ’» System Requirements

- **GPU Required**: No
- **Minimum RAM**: 4GB
- **Recommended RAM**: 8GB  
- **CPU Cores**: 2+ recommended
- **Storage**: 50GB+ for PDF storage
- **Network**: Stable internet for API calls

## ðŸ³ Docker Execution

### Quick Start:
```bash
# Clone and setup
cd pipelines/01-data_ingestion/03-ingest_pdf_corpus

# Configure environment
cp .env.example .env
# Edit .env with your API keys and email

# Run automated pipeline
docker compose --profile automated up --build
```

### Custom Configuration:
```bash
# Override specific timeouts
docker compose run -e DOWNLOAD_TIMEOUT=60 openalex-fetcher

# Run with custom rate limits
docker compose run -e RATE_LIMIT_DELAY=20 core-fetcher

# Manual downloads only
docker compose --profile stage4 up
```

## ðŸ“ Folder Structure

```
03-ingest_pdf_corpus/
â”œâ”€â”€ Dockerfile                          # Container configuration
â”œâ”€â”€ docker-compose.yaml                 # Multi-stage orchestration
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .env.example                        # Environment template
â”œâ”€â”€ .env                                # Your configuration (not in git)
â”œâ”€â”€ README.md                           # This documentation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01-openalex_fetcher.py          # OpenAlex PDF downloader
â”‚   â”œâ”€â”€ 02-core_fetcher.py              # CORE repository downloader  
â”‚   â”œâ”€â”€ 03-other_fetcher.py             # Additional sources (Unpaywall, PMC, arXiv)
â”‚   â”œâ”€â”€ 04-manual_download_helper.py    # Interactive manual download assistant
â”‚   â”œâ”€â”€ 04-run_manual_helper.sh         # Host-based manual helper script
â”‚   â””â”€â”€ 05-validate_dedupe.py           # PDF validation and deduplication
â”œâ”€â”€ output/                             # All downloaded and processed PDFs
â”‚   â”œâ”€â”€ openalex/                       # PDFs from OpenAlex
â”‚   â”œâ”€â”€ core/                           # PDFs from CORE
â”‚   â”œâ”€â”€ other/                          # PDFs from other sources
â”‚   â”œâ”€â”€ manual/                         # Manually downloaded PDFs
â”‚   â”œâ”€â”€ manual_renamed/                 # Renamed manual PDFs
â”‚   â”œâ”€â”€ valid_pdfs/                     # Final validated unique PDFs
â”‚   â”œâ”€â”€ invalid_pdfs/                   # Corrupted or problematic PDFs
â”‚   â”œâ”€â”€ duplicate_pdfs/                 # Removed duplicate PDFs
â”‚   â”œâ”€â”€ manual_download.txt             # URLs needing manual download
â”‚   â”œâ”€â”€ batch_urls.html                 # Web interface for manual downloads
â”‚   â””â”€â”€ validation_report.txt           # Final processing report
â””â”€â”€ input/                              # Input data (mounted from previous pipeline)
    â””â”€â”€ publications.txt                # Publications to download (from 01-02)
```

## ðŸ”§ Configuration

### Environment Variables (.env):
```env
# Required: Publication list from previous pipeline
PUBLICATIONS_FILE=/app/data/publications.txt

# Required: Your email for API identification
EMAIL=your.email@university.edu

# API Keys (highly recommended for better rate limits)
OPENALEX_TOKEN=your_openalex_token_here
CORE_API_KEY=your_core_api_key_here

# Optional: Timeout configurations
DOWNLOAD_TIMEOUT=30
REQUEST_TIMEOUT=15
RATE_LIMIT_DELAY=12

# Optional: Quality thresholds
MIN_FILE_SIZE=1000
SIMILARITY_THRESHOLD=0.8
```

### Getting API Keys:
- **OpenAlex**: Free at [https://openalex.org/](https://openalex.org/)
- **CORE**: Free academic key at [https://core.ac.uk/services/api/](https://core.ac.uk/services/api/)

## ðŸ“Š Output Format

### Final PDFs:
- **Location**: `./output/valid_pdfs/`
- **Naming**: `{doi}_{title_words}_{year}.pdf`
- **Quality**: Validated, deduplicated, readable

### Metadata Files:
```
manual_download.txt     # URLs that need manual download
validation_report.txt   # Processing statistics and results
batch_urls.html        # Web interface for manual downloads
```

### Source Tracking:
Each source maintains publication tracking files:
- `./output/openalex/publications/publications_openalex.txt`
- `./output/core/publications/publications_core.txt`  
- `./output/other/publications/publications_other.txt`

## ðŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| API rate limiting | Reduce concurrent requests, increase delays |
| Out of storage | Check available disk space, clean output dirs |
| Invalid API keys | Verify keys in .env file, check API quotas |
| Manual downloads needed | Use `batch_urls.html` or run manual helper |
| PDF validation failures | Check PyPDF2 compatibility, file corruption |
| Network timeouts | Increase timeout values, check connection |
| Docker build fails | Update requirements.txt, rebuild with --no-cache |

### Common Error Messages:
- `"Rate limited (429)"` â†’ Wait for rate limit reset or get API key
- `"PDF read error"` â†’ File corrupted, will be moved to invalid_pdfs/
- `"No PDF URLs found"` â†’ Paper not available in open access
- `"File too small"` â†’ Download incomplete, will retry

## ðŸ“ˆ Expected Performance

### Processing Times (typical):
- **1000 publications**: 2-4 hours (automated stages)
- **Manual downloads**: Variable (depends on user)
- **Validation**: 10-30 minutes
- **Total pipeline**: 3-6 hours for 1000 papers

### Success Rates (typical):
- **OpenAlex**: 40-60% of publications found
- **CORE**: 20-40% of publications found  
- **Other sources**: 10-30% of publications found
- **Overall**: 60-80% PDF acquisition rate

### Resource Usage:
- **Peak RAM**: 2-4GB during validation
- **Storage**: ~1GB per 100 PDFs
- **Network**: ~50MB per successful download

## ðŸ“ Notes

- **Rate Limits**: CORE has strict limits (10 req/min academic), OpenAlex is more generous
- **Manual Step**: Some PDFs require manual download due to CAPTCHAs or login requirements
- **Quality Control**: All PDFs validated for corruption, minimum pages, readable text
- **Deduplication**: Uses multiple methods (file hash, DOI matching, text similarity)
- **Resumability**: Pipeline can be resumed from any stage using Docker profiles
- **Source Priority**: OpenAlex > PMC > CORE > arXiv > Other sources

---
âœ¨ Pipeline ready for execution - Follow the quick start guide to begin PDF corpus creation